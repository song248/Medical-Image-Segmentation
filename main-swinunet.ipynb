{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a383241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import natsort\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5210a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수 설정\n",
    "sys.path.append(\"..\")\n",
    "module_names=['SwinUNet']\n",
    "for module_name in module_names:\n",
    "    exec('from models.'+module_name+' import *')\n",
    "model_names= ['SwinUNet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e78569",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.7\n",
    "in_channels = 3\n",
    "\n",
    "number_of_classes = 1\n",
    "loss_function = 'Tversky Focal Loss'\n",
    "Optimizers = ['AdamW']\n",
    "LRs = [1e-3]\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "iterations = [1,1]\n",
    "devices = [0,1] \n",
    "Target_Datasets = ['reflex_mask']\n",
    "Dataset_dir = 'Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64345885",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-11T03:00:36.848Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def control_random_seed(seed, pytorch=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available()==True:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "    except:\n",
    "        pass\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def imread_kor ( filePath, mode=cv2.IMREAD_UNCHANGED ) : \n",
    "    stream = open( filePath.encode(\"utf-8\") , \"rb\") \n",
    "    bytes = bytearray(stream.read()) \n",
    "    numpyArray = np.asarray(bytes, dtype=np.uint8)\n",
    "    return cv2.imdecode(numpyArray , mode)\n",
    "def imwrite_kor(filename, img, params=None): \n",
    "    try: \n",
    "        ext = os.path.splitext(filename)[1] \n",
    "        result, n = cv2.imencode(ext, img, params) \n",
    "        if result:\n",
    "            with open(filename, mode='w+b') as f: \n",
    "                n.tofile(f) \n",
    "                return True\n",
    "        else: \n",
    "            return False \n",
    "    except Exception as e: \n",
    "        print(e) \n",
    "        return False\n",
    "class ImagesDataset(Dataset):\n",
    "    def __init__(self, image_path_list, target_path_list, transform=None):\n",
    "        self.image_path_list = image_path_list\n",
    "        self.target_path_list = target_path_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_path_list[idx]\n",
    "        mask_path = self.target_path_list[idx]\n",
    "        image = np.load(image_path) \n",
    "        mask = imread_kor(mask_path)\n",
    "        if image.shape[0]!=512:\n",
    "            print(image.shape, mask.shape, image_path, mask_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        mask[mask > 0] = 1\n",
    "        return image, mask, image_path\n",
    "def adjust_learning_rate(optimizer, epoch, lr):\n",
    "    lr = lr * (0.5 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "### losses & accuracy ### https://github.com/Jo-dsa/SemanticSeg/blob/master/src/utils.py\n",
    "def tversky_index(yhat, ytrue, alpha=0.3, beta=0.7, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Computes Tversky index\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks\n",
    "        ytrue (Tensor): targets masks\n",
    "        alpha (Float): weight for False positive\n",
    "        beta (Float): weight for False negative\n",
    "                    `` alpha and beta control the magnitude of penalties and should sum to 1``\n",
    "        epsilon (Float): smoothing value to avoid division by 0\n",
    "    output:\n",
    "        tversky index value\n",
    "    \"\"\"\n",
    "    TP = torch.sum(yhat * ytrue, (1,2,3))\n",
    "    FP = torch.sum((1. - ytrue) * yhat, (1,2,3))\n",
    "    FN = torch.sum((1. - yhat) * ytrue, (1,2,3))\n",
    "    \n",
    "    return TP/(TP + alpha * FP + beta * FN + epsilon)\n",
    "\n",
    "\n",
    "def tversky_focal_loss(yhat, ytrue, alpha=0.7, beta=0.3, gamma=0.75):\n",
    "    \"\"\"\n",
    "    Computes tversky focal loss for highly umbalanced data\n",
    "    https://arxiv.org/pdf/1810.07842.pdf\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks\n",
    "        ytrue (Tensor): targets masks\n",
    "        alpha (Float): weight for False positive\n",
    "        beta (Float): weight for False negative\n",
    "                    `` alpha and beta control the magnitude of penalties and should sum to 1``\n",
    "        gamma (Float): focal parameter\n",
    "                    ``control the balance between easy background and hard ROI training examples``\n",
    "    output:\n",
    "        tversky focal loss value with `mean` reduction\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.mean(torch.pow(1 - tversky_index(yhat, ytrue, alpha, beta), gamma))\n",
    "\n",
    "def focal_loss(yhat, ytrue, alpha=0.75, gamma=2):\n",
    "    \"\"\"\n",
    "    Computes α-balanced focal loss from FAIR\n",
    "    https://arxiv.org/pdf/1708.02002v2.pdf\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks\n",
    "        ytrue (Tensor): targets masks\n",
    "        alpha (Float): weight to balance Cross entropy value\n",
    "        gamma (Float): focal parameter\n",
    "    output:\n",
    "        loss value with `mean` reduction\n",
    "    \"\"\"\n",
    "\n",
    "    # compute the actual focal loss\n",
    "    focal = -alpha * torch.pow(1. - yhat, gamma) * torch.log(yhat)\n",
    "    f_loss = torch.sum(ytrue * focal, dim=1)\n",
    "\n",
    "    return torch.mean(f_loss)\n",
    "\n",
    "def Intersection_over_Union(yhat, ytrue, threshold=0.5, epsilon=1e-6, nan_process = 'remove'):\n",
    "    \"\"\"\n",
    "    Computes Intersection over Union metric\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks (batch_size, 1, height, width)\n",
    "        ytrue (Tensor): targets masks (batch_size, 1, height, width)\n",
    "        threshold (Float): threshold for pixel classification\n",
    "        epsilon (Float): smoothing parameter for numerical stability\n",
    "    output:\n",
    "        iou value with `mean` reduction\n",
    "    \"\"\"\n",
    "    intersection = ((yhat>threshold).long() & ytrue.long()).float().sum((1,2,3))\n",
    "    union = ((yhat>threshold).long() | ytrue.long()).float().sum((1,2,3))\n",
    "    if nan_process == 'remove': # if sum of true == 0, remove\n",
    "        sum_bool = torch.sum(torch.flatten(ytrue,1),1).bool()\n",
    "        iou =(intersection/(union))#.reshape(intersection.shape[0],-1)\n",
    "        iou = torch.nanmean(iou,dim=0)\n",
    "        if torch.isnan(iou):\n",
    "            return 0\n",
    "        return (torch.mean(iou)).item()\n",
    "\n",
    "def Dice_Coefficient(yhat, ytrue, epsilon=1e-6, nan_process = 'remove'):\n",
    "    \"\"\"\n",
    "    Computes a soft Dice Loss\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks (batch_size, 1, height, width)\n",
    "        ytrue (Tensor): targets masks (batch_size, 1, height, width)\n",
    "        epsilon (Float): smoothing value to avoid division by 0\n",
    "    output:\n",
    "        DL value with `mean` reduction\n",
    "    \"\"\"\n",
    "    # compute Dice components\n",
    "    intersection = torch.sum(yhat * ytrue, (1,2,3))\n",
    "    cardinal = torch.sum(yhat + ytrue, (1,2,3))\n",
    "    if nan_process == 'remove': # if sum of true == 0, remove\n",
    "        sum_bool = torch.sum(torch.flatten(ytrue,1),1).bool()\n",
    "        dice = (2 * intersection / (cardinal))#.reshape(intersection.shape[0],-1)\n",
    "        dice = torch.nanmean(dice,dim=0)\n",
    "        if torch.isnan(dice):\n",
    "            return 0\n",
    "        return (torch.mean(dice)).item()\n",
    "def train(train_loader, epoch, \\\n",
    "          model, criterion, optimizer, device\n",
    "          ):\n",
    "    model.train()\n",
    "    limit=0\n",
    "    train_losses=AverageMeter()\n",
    "    for i, (input, target, _) in enumerate(train_loader):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        output = nn.Sigmoid()(model(input))\n",
    "        loss = criterion(output,target).float()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        train_losses.update(loss.detach().cpu().numpy(),input.shape[0])\n",
    "    Train_Loss=np.round(train_losses.avg,6)\n",
    "    return Train_Loss\n",
    "def validate(validation_loader, \n",
    "          model, criterion, device,\n",
    "        model_path=False,\n",
    "             return_image_paths=False,\n",
    "          ):\n",
    "    if model_path!=False:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    for i, (input, target, image_path) in enumerate(validation_loader):\n",
    "        input =input.to(device)\n",
    "        target = target.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = nn.Sigmoid()(model(input))\n",
    "        if i==0:\n",
    "            targets=target\n",
    "            outputs=output\n",
    "            if return_image_paths==True:\n",
    "                image_paths = image_path\n",
    "        else:\n",
    "            targets=torch.cat((targets,target))\n",
    "            outputs=torch.cat((outputs,output),axis=0)\n",
    "            if return_image_paths==True:\n",
    "                image_paths += image_path\n",
    "    if return_image_paths==True:\n",
    "        return outputs, targets, image_paths\n",
    "    return outputs, targets\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)\n",
    "\n",
    "def Do_Experiment(iteration, model_name, model, Optimizer, lr,  number_of_classes, epochs, Metrics,df,device):\n",
    "    train_dataset = ImagesDataset(train_image_path_list, train_target_path_list, transform)\n",
    "    validation_dataset = ImagesDataset(validation_image_path_list, validation_target_path_list, transform)\n",
    "    test_dataset = ImagesDataset(test_image_path_list, test_target_path_list, transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size,\n",
    "    num_workers=num_workers, pin_memory=pin_memory\n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, batch_size=batch_size, \n",
    "        num_workers=num_workers, pin_memory=pin_memory\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, \n",
    "        num_workers=num_workers, pin_memory=pin_memory\n",
    "    )\n",
    "    start = timeit.default_timer()\n",
    "    train_bool=True\n",
    "    test_bool=True\n",
    "    if loss_function == 'Tversky Focal Loss':\n",
    "        criterion=tversky_focal_loss\n",
    "    else:\n",
    "        criterion=torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    if Optimizer=='Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif Optimizer == 'SGD':\n",
    "        momentum = 0.9\n",
    "        weight_decay = 1e-4\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum ,weight_decay=weight_decay)\n",
    "    elif Optimizer =='AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    save_dir='saved_model'\n",
    "    try:\n",
    "        os.mkdir(save_dir)\n",
    "    except:\n",
    "        pass\n",
    "    if train_bool:\n",
    "        now = datetime.now()\n",
    "        Train_date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Training Start Time:',Train_date)\n",
    "        best=9999\n",
    "        best_epoch=1\n",
    "        Early_Stop=0\n",
    "        Early_Stop_Start=30\n",
    "        train_start_time = timeit.default_timer()\n",
    "        Train_Losses=[]\n",
    "        Validation_Losses=[]\n",
    "        for epoch in range(1, epochs+1):\n",
    "            adjust_learning_rate(optimizer, epoch, lr)\n",
    "            Train_Loss = train(train_loader, epoch, \n",
    "              model, criterion, optimizer, device\n",
    "              )\n",
    "            outputs, targets  \\\n",
    "            = validate(validation_loader, \n",
    "              model, criterion, device\n",
    "              )\n",
    "            Loss = np.round(criterion(outputs,targets).cpu().numpy(),6)            \n",
    "            iou = np.round(Intersection_over_Union(outputs, targets),3)\n",
    "            dice = np.round(Dice_Coefficient(outputs, targets),3)\n",
    "            now = datetime.now()\n",
    "            date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "            print(str(epoch)+'EP('+date+'):',end=' ')\n",
    "            print('T_Loss: ' + str(Train_Loss), end=' ')\n",
    "            print('V_Loss: ' + str(Loss), end=' ')\n",
    "            print('IoU: ' + str(iou), end=' ')\n",
    "            print('Dice: ' + str(dice), end='\\n')\n",
    "                        \n",
    "            if Loss<best:\n",
    "                torch.save(model.state_dict(), save_dir+'/'+model_name+'_'+Dataset_name+'.pt')\n",
    "                best_epoch = epoch\n",
    "                best = Loss\n",
    "                print('Best Epoch:',best_epoch,'Loss:',Loss)\n",
    "        train_stop_time = timeit.default_timer()\n",
    "    if test_bool:\n",
    "        now = datetime.now()\n",
    "        date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Test Start Time:',date)\n",
    "        outputs, targets, image_paths \\\n",
    "            = validate(test_loader, \n",
    "              model, criterion, device,\n",
    "            model_path=save_dir+'/'+model_name+'_'+Dataset_name+'.pt',\n",
    "                       return_image_paths=True\n",
    "              )        \n",
    "        Loss = np.round(criterion(outputs,targets).cpu().numpy(),6)\n",
    "        iou = np.round(Intersection_over_Union(outputs, targets),3)\n",
    "        dice = np.round(Dice_Coefficient(outputs, targets),3)\n",
    "\n",
    "        now = datetime.now()\n",
    "        date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Best Epoch:',best_epoch)\n",
    "        print('Test('+date+'): '+'Loss: ' + str(Loss),end=' ')\n",
    "        print('IoU: ' + str(iou), end=' ')\n",
    "        print('Dice: ' + str(dice), end='\\n')                    \n",
    "                            \n",
    "        stop = timeit.default_timer()\n",
    "        m, s = divmod((train_stop_time - train_start_time)/epoch, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        Time_per_Epoch = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "        m, s = divmod(stop - start, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        Time = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "        print(Time)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        total_params = format(total_params , ',')\n",
    "        Performances = [iteration, Dataset_name, model_name, loss_function, lr, batch_size, epochs,   Loss, iou, dice, total_params,Time, best_epoch, Time_per_Epoch]\n",
    "        df = df.append(pd.Series(Performances, index=df.columns), ignore_index=True)\n",
    "    now = datetime.now()\n",
    "    date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "    print('End',date)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c190b414-5b97-49ff-a002-47c4192144d5",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Start Time: 230620_203636\n",
      "Batch Size: 32\n",
      "Train Size 0.7\n",
      "1/1 reflex_mask\n",
      "SwinUNet\n",
      "LR: 0.001\n",
      "(128, 128)\n",
      "H: 128 W: 128\n",
      "-----mask\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 128, 128, 1])\n",
      "******************************\n",
      "torch.Size([1, 128, 128, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml/anaconda3/envs/LSH_Python_3.8/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 18, 7, 18, 7, 1]' is invalid for input of size 16384",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m control_random_seed(seed)\n\u001b[1;32m     75\u001b[0m in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 76\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mstr_to_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(devices[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(devices)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/song/Medical_Image_Segmentation_2023/models/SwinUNet.py:1023\u001b[0m, in \u001b[0;36mSwinUNet.__init__\u001b[0;34m(self, in_chans, num_classes, config, img_size, zero_head, vis)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_head \u001b[38;5;241m=\u001b[39m zero_head\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m-> 1023\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswin_unet \u001b[38;5;241m=\u001b[39m \u001b[43mSwinTransformerSys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSWIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m                        \u001b[49m\u001b[43min_chans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSWIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIN_CHANS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSWIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEMBED_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdepths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSWIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEPTHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSWIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNUM_HEADS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSWIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWINDOW_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSWIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMLP_RATIO\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSWIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQKV_BIAS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mqk_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSWIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQK_SCALE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDROP_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdrop_path_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDROP_PATH_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSWIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpatch_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSWIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPATCH_NORM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m                        \u001b[49m\u001b[43muse_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUSE_CHECKPOINT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/song/Medical_Image_Segmentation_2023/models/SwinUNet.py:637\u001b[0m, in \u001b[0;36mSwinTransformerSys.__init__\u001b[0;34m(self, in_chans, num_classes, img_size, patch_size, embed_dim, depths, depths_decoder, num_heads, window_size, mlp_ratio, qkv_bias, qk_scale, drop_rate, attn_drop_rate, drop_path_rate, norm_layer, ape, patch_norm, use_checkpoint, final_upsample, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m--> 637\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43mBasicLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi_layer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m                       \u001b[49m\u001b[43minput_resolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpatches_resolution\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi_layer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mpatches_resolution\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi_layer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqkv_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqk_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqk_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_drop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_drop_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdepths\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdepths\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi_layer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdownsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPatchMerging\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_layer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m                       \u001b[49m\u001b[43muse_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(layer)\n\u001b[1;32m    652\u001b[0m \u001b[38;5;66;03m# build decoder layers\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/song/Medical_Image_Segmentation_2023/models/SwinUNet.py:427\u001b[0m, in \u001b[0;36mBasicLayer.__init__\u001b[0;34m(self, dim, input_resolution, depth, num_heads, window_size, mlp_ratio, qkv_bias, qk_scale, drop, attn_drop, drop_path, norm_layer, downsample, use_checkpoint)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_checkpoint \u001b[38;5;241m=\u001b[39m use_checkpoint\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# build blocks\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m    428\u001b[0m     SwinTransformerBlock(dim\u001b[38;5;241m=\u001b[39mdim, input_resolution\u001b[38;5;241m=\u001b[39minput_resolution,\n\u001b[1;32m    429\u001b[0m                          num_heads\u001b[38;5;241m=\u001b[39mnum_heads, window_size\u001b[38;5;241m=\u001b[39mwindow_size,\n\u001b[1;32m    430\u001b[0m                          shift_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m window_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    431\u001b[0m                          mlp_ratio\u001b[38;5;241m=\u001b[39mmlp_ratio,\n\u001b[1;32m    432\u001b[0m                          qkv_bias\u001b[38;5;241m=\u001b[39mqkv_bias, qk_scale\u001b[38;5;241m=\u001b[39mqk_scale,\n\u001b[1;32m    433\u001b[0m                          drop\u001b[38;5;241m=\u001b[39mdrop, attn_drop\u001b[38;5;241m=\u001b[39mattn_drop,\n\u001b[1;32m    434\u001b[0m                          drop_path\u001b[38;5;241m=\u001b[39mdrop_path[i] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(drop_path, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m drop_path,\n\u001b[1;32m    435\u001b[0m                          norm_layer\u001b[38;5;241m=\u001b[39mnorm_layer)\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)])\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# patch merging layer\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m downsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/song/Medical_Image_Segmentation_2023/models/SwinUNet.py:428\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_checkpoint \u001b[38;5;241m=\u001b[39m use_checkpoint\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# build blocks\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m--> 428\u001b[0m     \u001b[43mSwinTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_resolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_resolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mshift_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mqkv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqkv_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqk_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqk_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_drop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_drop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_path\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)])\n\u001b[1;32m    438\u001b[0m \u001b[38;5;66;03m# patch merging layer\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m downsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/song/Medical_Image_Segmentation_2023/models/SwinUNet.py:229\u001b[0m, in \u001b[0;36mSwinTransformerBlock.__init__\u001b[0;34m(self, dim, input_resolution, num_heads, window_size, shift_size, mlp_ratio, qkv_bias, qk_scale, drop, attn_drop, drop_path, act_layer, norm_layer)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(img_mask))\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mprint\u001b[39m(img_mask\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m--> 229\u001b[0m mask_windows \u001b[38;5;241m=\u001b[39m \u001b[43mwindow_partition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# nW, window_size, window_size, 1\u001b[39;00m\n\u001b[1;32m    230\u001b[0m mask_windows \u001b[38;5;241m=\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[1;32m    231\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/song/Medical_Image_Segmentation_2023/models/SwinUNet.py:44\u001b[0m, in \u001b[0;36mwindow_partition\u001b[0;34m(x, window_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     43\u001b[0m B, H, W, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m windows \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, window_size, window_size, C)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m windows\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 18, 7, 18, 7, 1]' is invalid for input of size 16384"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "Experiment_date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "print('Experiment Start Time:',Experiment_date)\n",
    "\n",
    "Metrics=['Iteration','Dataset','Model Name', 'Loss Function', 'LR', 'Batch size', '#Epochs',  'Loss', 'mIoU', 'mDice','Total Params','Train-Predction Time','Best Epoch','Time per Epoch']\n",
    "image_paths_dirs=[]\n",
    "target_dirs = [path  for path in natsort.natsorted(glob.glob(Dataset_dir+'/Masks/*'))]\n",
    "for target_dir in target_dirs:\n",
    "    image_paths_dirs.append(Dataset_dir+'/Originals/'+os.path.basename(target_dir).split('_')[0]+'_ori')\n",
    "df = pd.DataFrame(index=None, columns=Metrics)\n",
    "csv_file = False \n",
    "try:\n",
    "    if csv_file != False:\n",
    "        csv_file_for_modifying = 'Segmentation_Model_Comparison_Performance_220511_045617.csv'\n",
    "        df= pd.read_csv(csv_file_for_modifying, encoding='cp949')\n",
    "        df.to_csv('Segmentation_Model_Comparison_Performance_'+Experiment_date+'.csv', index=False, header=True, encoding=\"cp949\")\n",
    "except:\n",
    "    df = pd.DataFrame(index=None, columns=Metrics)\n",
    "print('Batch Size:',batch_size)\n",
    "print('Train Size',train_size)\n",
    "Dataset_Number=0\n",
    "for image_paths_dir, target_dir in zip(image_paths_dirs, target_dirs):\n",
    "    Dataset_name = os.path.basename(target_dir)\n",
    "    if Dataset_name not in Target_Datasets:\n",
    "        continue\n",
    "    Dataset_Number+=1\n",
    "    print(str(Dataset_Number)+'/'+str(len(Target_Datasets)), Dataset_name)\n",
    "    for iteration in range(iterations[0], iterations[1]+1):\n",
    "        seed=iteration    \n",
    "        for model_name in model_names:\n",
    "            if len(df[(df['Dataset'] ==Dataset_name) & (df['Model Name']== model_name) & (df['Iteration']== iteration)])>0:\n",
    "                continue\n",
    "            image_path_list=natsort.natsorted(glob.glob(image_paths_dir+'/*'))\n",
    "            target_path_list=[]\n",
    "            for image_path in image_path_list:\n",
    "                target_path_list.append(target_dir+'/'+os.path.basename(image_path).replace('npy','tif'))\n",
    "            transform = transforms.Compose([\n",
    "                        transforms.ToPILImage(),\n",
    "                        transforms.Resize((224,224)),\n",
    "                        transforms.ToTensor(),\n",
    "                ])\n",
    "            num_workers=4\n",
    "            shuffle=True\n",
    "            pin_memory=True\n",
    "            num_dataset = len(target_path_list)\n",
    "            indices = list(range(num_dataset))\n",
    "            split1=int(train_size*num_dataset)\n",
    "            split2=int((train_size+(1-train_size)/2)*num_dataset)\n",
    "            control_random_seed(seed)\n",
    "            if shuffle:\n",
    "                np.random.shuffle(indices)\n",
    "            train_idx, validation_idx, test_idx = indices[:split1], indices[split1:split2], indices[split2:]\n",
    "            train_image_path_list=[]\n",
    "            train_target_path_list=[]\n",
    "            validation_image_path_list=[]\n",
    "            validation_target_path_list=[]\n",
    "            test_image_path_list=[]\n",
    "            test_target_path_list=[]\n",
    "            for i, index in enumerate(indices):\n",
    "                if i<split1:\n",
    "                    train_image_path_list.append(image_path_list[index])\n",
    "                    train_target_path_list.append(target_path_list[index])\n",
    "                elif split1<=i and i<split2:\n",
    "                    validation_image_path_list.append(image_path_list[index])\n",
    "                    validation_target_path_list.append(target_path_list[index])\n",
    "                else:\n",
    "                    test_image_path_list.append(image_path_list[index])\n",
    "                    test_target_path_list.append(target_path_list[index])\n",
    "\n",
    "            print(model_name)\n",
    "            for Optimizer in Optimizers :\n",
    "                for lr in LRs:\n",
    "                    print('LR:',lr)\n",
    "                    control_random_seed(seed)\n",
    "                    in_channels=1\n",
    "                    model=str_to_class(model_name)(in_channels, number_of_classes)\n",
    "                    device = torch.device(\"cuda:\"+str(devices[0]))\n",
    "                    if len(devices)>1:\n",
    "                        model = torch.nn.DataParallel(model, device_ids = devices ).to(device)\n",
    "                    else:\n",
    "                        model = model.to(device)\n",
    "                    df = Do_Experiment(iteration, model_name, model, Optimizer, lr,  number_of_classes, epochs, Metrics,df,device)\n",
    "                    try:\n",
    "                        df.to_csv('Segmentation_Model_Comparison_Performance_'+Experiment_date+'.csv', index=False, header=True, encoding=\"cp949\")\n",
    "                    except:\n",
    "                        now = datetime.now()\n",
    "                        tmp_date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "                        df.to_csv('Segmentation_Model_Comparison_Performance_'+Experiment_date+'_'+tmp_date+'_tmp'+'.csv', index=False, header=True, encoding=\"cp949\")\n",
    "import os\n",
    "print('End')\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4c5bbd-261d-49e4-ac4c-46c456b646c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
