{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b32f1187-9730-4acd-bf18-31f482019674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "#torch._dynamo.list_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d58f7a5d-e965-4899-be63-6c65524d20fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '_dynamo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlog_level\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute '_dynamo'"
     ]
    }
   ],
   "source": [
    "torch._dynamo.config.log_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff41c442-57f1-4d51-bbc1-d375567d6d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Start Time: 230414_085338\n",
      "Batch Size: 16\n",
      "Train Size 0.7\n",
      "1/1 co2_내직근_left\n",
      "UNet\n",
      "LR: 0.001\n",
      "Training Start Time: 230414_085339\n",
      "1EP(230414_085347): T_Loss: 0.997598 V_Loss: 0.997327 IoU: 0.117 Dice: 0.005\n",
      "Best Epoch: 1 Loss: 0.997327\n",
      "2EP(230414_085353): T_Loss: 0.996508 V_Loss: 0.996375 IoU: 0.088 Dice: 0.007\n",
      "Best Epoch: 2 Loss: 0.996375\n",
      "3EP(230414_085359): T_Loss: 0.995088 V_Loss: 0.995245 IoU: 0.084 Dice: 0.009\n",
      "Best Epoch: 3 Loss: 0.995245\n",
      "4EP(230414_085405): T_Loss: 0.99252 V_Loss: 0.997335 IoU: 0.01 Dice: 0.005\n",
      "5EP(230414_085411): T_Loss: 0.984811 V_Loss: 0.943839 IoU: 0.202 Dice: 0.101\n",
      "Best Epoch: 5 Loss: 0.943839\n",
      "6EP(230414_085417): T_Loss: 0.936872 V_Loss: 0.849536 IoU: 0.345 Dice: 0.252\n",
      "Best Epoch: 6 Loss: 0.849536\n",
      "7EP(230414_085424): T_Loss: 0.741815 V_Loss: 0.448807 IoU: 0.749 Dice: 0.701\n",
      "Best Epoch: 7 Loss: 0.448807\n",
      "8EP(230414_085430): T_Loss: 0.403391 V_Loss: 0.415887 IoU: 0.763 Dice: 0.714\n",
      "Best Epoch: 8 Loss: 0.415887\n",
      "9EP(230414_085436): T_Loss: 0.243004 V_Loss: 0.182959 IoU: 0.818 Dice: 0.876\n",
      "Best Epoch: 9 Loss: 0.182959\n",
      "10EP(230414_085442): T_Loss: 0.185067 V_Loss: 0.171088 IoU: 0.802 Dice: 0.875\n",
      "Best Epoch: 10 Loss: 0.171088\n",
      "11EP(230414_085448): T_Loss: 0.16508 V_Loss: 0.207083 IoU: 0.841 Dice: 0.896\n",
      "12EP(230414_085454): T_Loss: 0.153786 V_Loss: 0.129763 IoU: 0.87 Dice: 0.921\n",
      "Best Epoch: 12 Loss: 0.129763\n",
      "13EP(230414_085500): T_Loss: 0.139122 V_Loss: 0.127249 IoU: 0.875 Dice: 0.924\n",
      "Best Epoch: 13 Loss: 0.127249\n",
      "14EP(230414_085507): T_Loss: 0.13644 V_Loss: 0.133762 IoU: 0.845 Dice: 0.908\n",
      "15EP(230414_085513): T_Loss: 0.125465 V_Loss: 0.130718 IoU: 0.881 Dice: 0.928\n",
      "16EP(230414_085519): T_Loss: 0.118686 V_Loss: 0.114732 IoU: 0.879 Dice: 0.928\n",
      "Best Epoch: 16 Loss: 0.114732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ml/anaconda3/envs/LSH_Torch_2.0/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/ml/anaconda3/envs/LSH_Torch_2.0/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/ml/anaconda3/envs/LSH_Torch_2.0/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/ml/anaconda3/envs/LSH_Torch_2.0/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 446\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 446\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mDo_Experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mnumber_of_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSegmentation_Model_Comparison_Performance_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mExperiment_date\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcp949\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 303\u001b[0m, in \u001b[0;36mDo_Experiment\u001b[0;34m(iteration, model_name, model, Optimizer, lr, number_of_classes, epochs, Metrics, df, device)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    302\u001b[0m     adjust_learning_rate(optimizer, epoch, lr)\n\u001b[0;32m--> 303\u001b[0m     Train_Loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     outputs, targets  \\\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;241m=\u001b[39m validate(validation_loader, \n\u001b[1;32m    308\u001b[0m       model, criterion, device\n\u001b[1;32m    309\u001b[0m       )\n\u001b[1;32m    310\u001b[0m     Loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(criterion(outputs,targets)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\u001b[38;5;241m6\u001b[39m)            \n",
      "Cell \u001b[0;32mIn[2], line 209\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, epoch, model, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m    207\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    208\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()  \n\u001b[0;32m--> 209\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy(),\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    210\u001b[0m Train_Loss\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mround(train_losses\u001b[38;5;241m.\u001b[39mavg,\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Train_Loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import natsort\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "module_names=['UNet']\n",
    "for  module_name in module_names:\n",
    "    exec('from models.'+module_name+' import *')\n",
    "#model_names= ['SegNet','SegNet_with_Skip_Connection']\n",
    "model_names= ['UNet']\n",
    "train_size=0.7\n",
    "in_channels = 1\n",
    "number_of_classes=1\n",
    "loss_function = 'Tversky Focal Loss'\n",
    "Optimizers = ['AdamW']\n",
    "LRs = [1e-3]\n",
    "batch_size=16\n",
    "epochs=50\n",
    "iterations=[1,1]\n",
    "devices = [0,1] \n",
    "Target_Datasets = ['co2_내직근_left']\n",
    "Dataset_dir = '../Total_Datasets/Datasets-Segmentation'\n",
    "def control_random_seed(seed, pytorch=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available()==True:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "    except:\n",
    "        pass\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def imread_kor ( filePath, mode=cv2.IMREAD_UNCHANGED ) : \n",
    "    stream = open( filePath.encode(\"utf-8\") , \"rb\") \n",
    "    bytes = bytearray(stream.read()) \n",
    "    numpyArray = np.asarray(bytes, dtype=np.uint8)\n",
    "    return cv2.imdecode(numpyArray , mode)\n",
    "def imwrite_kor(filename, img, params=None): \n",
    "    try: \n",
    "        ext = os.path.splitext(filename)[1] \n",
    "        result, n = cv2.imencode(ext, img, params) \n",
    "        if result:\n",
    "            with open(filename, mode='w+b') as f: \n",
    "                n.tofile(f) \n",
    "                return True\n",
    "        else: \n",
    "            return False \n",
    "    except Exception as e: \n",
    "        print(e) \n",
    "        return False\n",
    "class ImagesDataset(Dataset):\n",
    "    def __init__(self, image_path_list, target_path_list, transform=None):\n",
    "        self.image_path_list = image_path_list\n",
    "        self.target_path_list = target_path_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_path_list[idx]\n",
    "        mask_path = self.target_path_list[idx]\n",
    "        image = imread_kor(image_path) \n",
    "        mask = imread_kor(mask_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        mask[mask > 0] = 1\n",
    "        return image, mask, image_path\n",
    "def adjust_learning_rate(optimizer, epoch, lr):\n",
    "    lr = lr * (0.5 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "### losses & accuracy ### https://github.com/Jo-dsa/SemanticSeg/blob/master/src/utils.py\n",
    "def tversky_index(yhat, ytrue, alpha=0.3, beta=0.7, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Computes Tversky index\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks\n",
    "        ytrue (Tensor): targets masks\n",
    "        alpha (Float): weight for False positive\n",
    "        beta (Float): weight for False negative\n",
    "                    `` alpha and beta control the magnitude of penalties and should sum to 1``\n",
    "        epsilon (Float): smoothing value to avoid division by 0\n",
    "    output:\n",
    "        tversky index value\n",
    "    \"\"\"\n",
    "    TP = torch.sum(yhat * ytrue, (1,2,3))\n",
    "    FP = torch.sum((1. - ytrue) * yhat, (1,2,3))\n",
    "    FN = torch.sum((1. - yhat) * ytrue, (1,2,3))\n",
    "    \n",
    "    return TP/(TP + alpha * FP + beta * FN + epsilon)\n",
    "def tversky_focal_loss(yhat, ytrue, alpha=0.7, beta=0.3, gamma=0.75):\n",
    "    \"\"\"\n",
    "    Computes tversky focal loss for highly umbalanced data\n",
    "    https://arxiv.org/pdf/1810.07842.pdf\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks\n",
    "        ytrue (Tensor): targets masks\n",
    "        alpha (Float): weight for False positive\n",
    "        beta (Float): weight for False negative\n",
    "                    `` alpha and beta control the magnitude of penalties and should sum to 1``\n",
    "        gamma (Float): focal parameter\n",
    "                    ``control the balance between easy background and hard ROI training examples``\n",
    "    output:\n",
    "        tversky focal loss value with `mean` reduction\n",
    "    \"\"\"\n",
    "\n",
    "    return torch.mean(torch.pow(1 - tversky_index(yhat, ytrue, alpha, beta), gamma))\n",
    "\n",
    "def focal_loss(yhat, ytrue, alpha=0.75, gamma=2):\n",
    "    \"\"\"\n",
    "    Computes α-balanced focal loss from FAIR\n",
    "    https://arxiv.org/pdf/1708.02002v2.pdf\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks\n",
    "        ytrue (Tensor): targets masks\n",
    "        alpha (Float): weight to balance Cross entropy value\n",
    "        gamma (Float): focal parameter\n",
    "    output:\n",
    "        loss value with `mean` reduction\n",
    "    \"\"\"\n",
    "\n",
    "    # compute the actual focal loss\n",
    "    focal = -alpha * torch.pow(1. - yhat, gamma) * torch.log(yhat)\n",
    "    f_loss = torch.sum(ytrue * focal, dim=1)\n",
    "\n",
    "    return torch.mean(f_loss)\n",
    "\n",
    "def Intersection_over_Union(yhat, ytrue, threshold=0.5, epsilon=1e-6, nan_process = 'remove'):\n",
    "    \"\"\"\n",
    "    Computes Intersection over Union metric\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks (batch_size, 1, height, width)\n",
    "        ytrue (Tensor): targets masks (batch_size, 1, height, width)\n",
    "        threshold (Float): threshold for pixel classification\n",
    "        epsilon (Float): smoothing parameter for numerical stability\n",
    "    output:\n",
    "        iou value with `mean` reduction\n",
    "    \"\"\"\n",
    "    intersection = ((yhat>threshold).long() & ytrue.long()).float().sum((1,2,3))\n",
    "    union = ((yhat>threshold).long() | ytrue.long()).float().sum((1,2,3))\n",
    "    if nan_process == 'remove': # if sum of true == 0, remove\n",
    "        sum_bool = torch.sum(torch.flatten(ytrue,1),1).bool()\n",
    "        iou =(intersection/(union))#.reshape(intersection.shape[0],-1)\n",
    "        iou = torch.nanmean(iou,dim=0)\n",
    "        if torch.isnan(iou):\n",
    "            return 0\n",
    "        return (torch.mean(iou)).item()\n",
    "\n",
    "def Dice_Coefficient(yhat, ytrue, epsilon=1e-6, nan_process = 'remove'):\n",
    "    \"\"\"\n",
    "    Computes a soft Dice Loss\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks (batch_size, 1, height, width)\n",
    "        ytrue (Tensor): targets masks (batch_size, 1, height, width)\n",
    "        epsilon (Float): smoothing value to avoid division by 0\n",
    "    output:\n",
    "        DL value with `mean` reduction\n",
    "    \"\"\"\n",
    "    # compute Dice components\n",
    "    intersection = torch.sum(yhat * ytrue, (1,2,3))\n",
    "    cardinal = torch.sum(yhat + ytrue, (1,2,3))\n",
    "    if nan_process == 'remove': # if sum of true == 0, remove\n",
    "        sum_bool = torch.sum(torch.flatten(ytrue,1),1).bool()\n",
    "        dice = (2 * intersection / (cardinal))#.reshape(intersection.shape[0],-1)\n",
    "        dice = torch.nanmean(dice,dim=0)\n",
    "        if torch.isnan(dice):\n",
    "            return 0\n",
    "        return (torch.mean(dice)).item()\n",
    "def train(train_loader, epoch, \\\n",
    "          model, criterion, optimizer, device\n",
    "          ):\n",
    "    model.train()\n",
    "    limit=0\n",
    "    train_losses=AverageMeter()\n",
    "    for i, (input, target, _) in enumerate(train_loader):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        output = nn.Sigmoid()(model(input))\n",
    "        loss = criterion(output,target).float()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        train_losses.update(loss.detach().cpu().numpy(),input.shape[0])\n",
    "    Train_Loss=np.round(train_losses.avg,6)\n",
    "    return Train_Loss\n",
    "def validate(validation_loader, \n",
    "          model, criterion, device,\n",
    "        model_path=False,\n",
    "             return_image_paths=False,\n",
    "          ):\n",
    "    if model_path!=False:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    for i, (input, target, image_path) in enumerate(validation_loader):\n",
    "        input =input.to(device)\n",
    "        target = target.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = nn.Sigmoid()(model(input))\n",
    "        if i==0:\n",
    "            targets=target\n",
    "            outputs=output\n",
    "            if return_image_paths==True:\n",
    "                image_paths = image_path\n",
    "        else:\n",
    "            targets=torch.cat((targets,target))\n",
    "            outputs=torch.cat((outputs,output),axis=0)\n",
    "            if return_image_paths==True:\n",
    "                image_paths += image_path\n",
    "    if return_image_paths==True:\n",
    "        return outputs, targets, image_paths\n",
    "    return outputs, targets\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)\n",
    "\n",
    "def Do_Experiment(iteration, model_name, model, Optimizer, lr,  number_of_classes, epochs, Metrics,df,device):\n",
    "    train_dataset = ImagesDataset(train_image_path_list, train_target_path_list, transform)\n",
    "    validation_dataset = ImagesDataset(validation_image_path_list, validation_target_path_list, transform)\n",
    "    test_dataset = ImagesDataset(test_image_path_list, test_target_path_list, transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size,\n",
    "    num_workers=num_workers, pin_memory=pin_memory\n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, batch_size=batch_size, \n",
    "        num_workers=num_workers, pin_memory=pin_memory\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, \n",
    "        num_workers=num_workers, pin_memory=pin_memory\n",
    "    )\n",
    "    start = timeit.default_timer()\n",
    "    train_bool=True\n",
    "    test_bool=True\n",
    "    if loss_function == 'Tversky Focal Loss':\n",
    "        criterion=tversky_focal_loss\n",
    "    if Optimizer=='Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif Optimizer == 'SGD':\n",
    "        momentum = 0.9\n",
    "        weight_decay = 1e-4\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum ,weight_decay=weight_decay)\n",
    "    elif Optimizer =='AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    save_dir='saved_model'\n",
    "    try:\n",
    "        os.mkdir(save_dir)\n",
    "    except:\n",
    "        pass\n",
    "    if train_bool:\n",
    "        now = datetime.now()\n",
    "        Train_date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Training Start Time:',Train_date)\n",
    "        best=9999\n",
    "        best_epoch=1\n",
    "        Early_Stop=0\n",
    "        Early_Stop_Start=30\n",
    "        train_start_time = timeit.default_timer()\n",
    "        Train_Losses=[]\n",
    "        Validation_Losses=[]\n",
    "        for epoch in range(1, epochs+1):\n",
    "            adjust_learning_rate(optimizer, epoch, lr)\n",
    "            Train_Loss = train(train_loader, epoch, \n",
    "              model, criterion, optimizer, device\n",
    "              )\n",
    "            outputs, targets  \\\n",
    "            = validate(validation_loader, \n",
    "              model, criterion, device\n",
    "              )\n",
    "            Loss = np.round(criterion(outputs,targets).cpu().numpy(),6)            \n",
    "            iou = np.round(Intersection_over_Union(outputs, targets),3)\n",
    "            dice = np.round(Dice_Coefficient(outputs, targets),3)\n",
    "            now = datetime.now()\n",
    "            date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "            print(str(epoch)+'EP('+date+'):',end=' ')\n",
    "            print('T_Loss: ' + str(Train_Loss), end=' ')\n",
    "            print('V_Loss: ' + str(Loss), end=' ')\n",
    "            print('IoU: ' + str(iou), end=' ')\n",
    "            print('Dice: ' + str(dice), end='\\n')\n",
    "                        \n",
    "            if Loss<best:\n",
    "                torch.save(model.state_dict(), save_dir+'/'+model_name+'_'+Dataset_name+'.pt')\n",
    "                best_epoch = epoch\n",
    "                best = Loss\n",
    "                print('Best Epoch:',best_epoch,'Loss:',Loss)\n",
    "        train_stop_time = timeit.default_timer()\n",
    "    if test_bool:\n",
    "        now = datetime.now()\n",
    "        date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Test Start Time:',date)\n",
    "        outputs, targets, image_paths \\\n",
    "            = validate(test_loader, \n",
    "              model, criterion, device,\n",
    "            model_path=save_dir+'/'+model_name+'_'+Dataset_name+'.pt',\n",
    "                       return_image_paths=True\n",
    "              )        \n",
    "        Loss = np.round(criterion(outputs,targets).cpu().numpy(),6)\n",
    "        iou = np.round(Intersection_over_Union(outputs, targets),3)\n",
    "        dice = np.round(Dice_Coefficient(outputs, targets),3)\n",
    "\n",
    "        now = datetime.now()\n",
    "        date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Best Epoch:',best_epoch)\n",
    "        print('Test('+date+'): '+'Loss: ' + str(Loss),end=' ')\n",
    "        print('IoU: ' + str(iou), end=' ')\n",
    "        print('Dice: ' + str(dice), end='\\n')                    \n",
    "                            \n",
    "        stop = timeit.default_timer()\n",
    "        m, s = divmod((train_stop_time - train_start_time)/epoch, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        Time_per_Epoch = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "        m, s = divmod(stop - start, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        Time = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "        print(Time)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        total_params = format(total_params , ',')\n",
    "        Performances = [iteration, Dataset_name, model_name, loss_function, lr, batch_size, epochs,   Loss, iou, dice, total_params,Time, best_epoch, Time_per_Epoch]\n",
    "        df = df.append(pd.Series(Performances, index=df.columns), ignore_index=True)\n",
    "    now = datetime.now()\n",
    "    date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "    print('End',date)\n",
    "    \n",
    "    return df\n",
    "now = datetime.now()\n",
    "Experiment_date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "print('Experiment Start Time:',Experiment_date)\n",
    "\n",
    "Metrics=['Iteration','Dataset','Model Name', 'Loss Function', 'LR', 'Batch size', '#Epochs',  'Loss', 'mIoU', 'mDice','Total Params','Train-Predction Time','Best Epoch','Time per Epoch']\n",
    "image_paths_dirs=[]\n",
    "target_dirs = [path  for path in natsort.natsorted(glob.glob(Dataset_dir+'/*')) if not '_ori' in path]\n",
    "for target_dir in target_dirs:\n",
    "    image_paths_dirs.append(Dataset_dir+'/'+os.path.basename(target_dir).split('_')[0]+'_ori')\n",
    "df = pd.DataFrame(index=None, columns=Metrics)\n",
    "csv_file = False \n",
    "try:\n",
    "    if csv_file != False:\n",
    "        csv_file_for_modifying = 'Segmentation_Model_Comparison_Performance_220511_045617.csv'\n",
    "        df= pd.read_csv(csv_file_for_modifying, encoding='cp949')\n",
    "        df.to_csv('Segmentation_Model_Comparison_Performance_'+Experiment_date+'.csv', index=False, header=True, encoding=\"cp949\")\n",
    "except:\n",
    "    df = pd.DataFrame(index=None, columns=Metrics)\n",
    "print('Batch Size:',batch_size)\n",
    "print('Train Size',train_size)\n",
    "Dataset_Number=0\n",
    "for image_paths_dir, target_dir in zip(image_paths_dirs, target_dirs):\n",
    "    Dataset_name = os.path.basename(target_dir)\n",
    "    if Dataset_name not in Target_Datasets:\n",
    "        continue\n",
    "    Dataset_Number+=1\n",
    "    print(str(Dataset_Number)+'/'+str(len(Target_Datasets)), Dataset_name)\n",
    "    for iteration in range(iterations[0], iterations[1]+1):\n",
    "        seed=iteration    \n",
    "        for model_name in model_names:\n",
    "            if len(df[(df['Dataset'] ==Dataset_name) & (df['Model Name']== model_name) & (df['Iteration']== iteration)])>0:\n",
    "                continue\n",
    "            image_path_list=natsort.natsorted(glob.glob(image_paths_dir+'/*'))\n",
    "            target_path_list=[]\n",
    "            for image_path in image_path_list:\n",
    "                target_path_list.append(glob.glob(target_dir+'/'+os.path.basename(image_path).split('_')[0]+'_*')[0])\n",
    "            transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                ])\n",
    "            num_workers=4\n",
    "            shuffle=True\n",
    "            pin_memory=True\n",
    "            num_dataset = len(target_path_list)\n",
    "            indices = list(range(num_dataset))\n",
    "            split1=int(train_size*num_dataset)\n",
    "            split2=int((train_size+(1-train_size)/2)*num_dataset)\n",
    "            control_random_seed(seed)\n",
    "            if shuffle:\n",
    "                np.random.shuffle(indices)\n",
    "            train_idx, validation_idx, test_idx = indices[:split1], indices[split1:split2], indices[split2:]\n",
    "            train_image_path_list=[]\n",
    "            train_target_path_list=[]\n",
    "            validation_image_path_list=[]\n",
    "            validation_target_path_list=[]\n",
    "            test_image_path_list=[]\n",
    "            test_target_path_list=[]\n",
    "            for i, index in enumerate(indices):\n",
    "                if i<split1:\n",
    "                    train_image_path_list.append(image_path_list[index])\n",
    "                    train_target_path_list.append(target_path_list[index])\n",
    "                elif split1<=i and i<split2:\n",
    "                    validation_image_path_list.append(image_path_list[index])\n",
    "                    validation_target_path_list.append(target_path_list[index])\n",
    "                else:\n",
    "                    test_image_path_list.append(image_path_list[index])\n",
    "                    test_target_path_list.append(target_path_list[index])\n",
    "\n",
    "            print(model_name)\n",
    "            for Optimizer in Optimizers :\n",
    "                for lr in LRs:\n",
    "                    print('LR:',lr)\n",
    "                    control_random_seed(seed)\n",
    "                    in_channels=1\n",
    "                    model=str_to_class(model_name)(in_channels, number_of_classes)\n",
    "                    device = torch.device(\"cuda:\"+str(devices[0]))\n",
    "                    if len(devices)>1:\n",
    "                        model = torch.nn.DataParallel(model, device_ids = devices ).to(device)\n",
    "                    else:\n",
    "                        model = model.to(device)\n",
    "                    df = Do_Experiment(iteration, model_name, model, Optimizer, lr,  number_of_classes, epochs, Metrics,df,device)\n",
    "                    try:\n",
    "                        df.to_csv('Segmentation_Model_Comparison_Performance_'+Experiment_date+'.csv', index=False, header=True, encoding=\"cp949\")\n",
    "                    except:\n",
    "                        now = datetime.now()\n",
    "                        tmp_date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "                        df.to_csv('Segmentation_Model_Comparison_Performance_'+Experiment_date+'_'+tmp_date+'_tmp'+'.csv', index=False, header=True, encoding=\"cp949\")\n",
    "import os\n",
    "print('End')\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61a5ec-1dd0-4a3f-92bd-c068c27d6634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c56d62-cb3e-423d-9f19-b621347d2cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSH_Torch_2.0",
   "language": "python",
   "name": "lsh_torch_2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
