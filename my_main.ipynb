{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef98ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52c2efe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import natsort, random, copy\n",
    "import os, glob, sys\n",
    "import cv2\n",
    "\n",
    "import time, timeit\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01d5f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb010a85",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 환경변수 설정\n",
    "sys.path.append(\"..\")\n",
    "module_names=['SwinUNet']\n",
    "for module_name in module_names:\n",
    "    exec('from models.'+module_name+' import *')\n",
    "model_names= ['SwinUNet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6908bd1f-0e43-425b-9840-838a14bd0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.7\n",
    "in_channels = 3\n",
    "\n",
    "number_of_classes = 1\n",
    "loss_function = 'Tversky Focal Loss'\n",
    "Optimizers = ['AdamW']\n",
    "LRs = [1e-3]\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "iterations = [1,10]\n",
    "devices = [0,3] \n",
    "Target_Datasets = ['reflex_mask']\n",
    "Dataset_dir = 'Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "763bb54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_random_seed(seed, pytorch=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available()==True:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "    except:\n",
    "        pass\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8233b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imread_kor ( filePath, mode=cv2.IMREAD_UNCHANGED ) : \n",
    "    stream = open( filePath.encode(\"utf-8\") , \"rb\") \n",
    "    bytes = bytearray(stream.read()) \n",
    "    numpyArray = np.asarray(bytes, dtype=np.uint8)\n",
    "    return cv2.imdecode(numpyArray , mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57124ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imwrite_kor(filename, img, params=None): \n",
    "    try: \n",
    "        ext = os.path.splitext(filename)[1] \n",
    "        result, n = cv2.imencode(ext, img, params) \n",
    "        if result:\n",
    "            with open(filename, mode='w+b') as f: \n",
    "                n.tofile(f) \n",
    "                return True\n",
    "        else: \n",
    "            return False \n",
    "    except Exception as e: \n",
    "        print(e) \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "487ce65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDataset(Dataset):\n",
    "    def __init__(self, image_path_list, target_path_list, transform=None):\n",
    "        self.image_path_list = image_path_list\n",
    "        self.target_path_list = target_path_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_path_list[idx]\n",
    "        mask_path = self.target_path_list[idx]\n",
    "        image = np.load(image_path) \n",
    "        mask = imread_kor(mask_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        mask[mask > 0] = 1\n",
    "        return image, mask, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c541f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, lr):\n",
    "    lr = lr * (0.5 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd6a5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "### losses & accuracy ### https://github.com/Jo-dsa/SemanticSeg/blob/master/src/utils.py\n",
    "def tversky_index(yhat, ytrue, alpha=0.3, beta=0.7, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Computes Tversky index\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks\n",
    "        ytrue (Tensor): targets masks\n",
    "        alpha (Float): weight for False positive\n",
    "        beta (Float): weight for False negative\n",
    "                    `` alpha and beta control the magnitude of penalties and should sum to 1``\n",
    "        epsilon (Float): smoothing value to avoid division by 0\n",
    "    output:\n",
    "        tversky index value\n",
    "    \"\"\"\n",
    "    TP = torch.sum(yhat * ytrue, (1,2,3))\n",
    "    FP = torch.sum((1. - ytrue) * yhat, (1,2,3))\n",
    "    FN = torch.sum((1. - yhat) * ytrue, (1,2,3))\n",
    "    \n",
    "    return TP/(TP + alpha * FP + beta * FN + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ddc2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tversky_focal_loss(yhat, ytrue, alpha=0.7, beta=0.3, gamma=0.75):\n",
    "    \"\"\"\n",
    "    Computes tversky focal loss for highly umbalanced data\n",
    "    https://arxiv.org/pdf/1810.07842.pdf\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks\n",
    "        ytrue (Tensor): targets masks\n",
    "        alpha (Float): weight for False positive\n",
    "        beta (Float): weight for False negative\n",
    "                    `` alpha and beta control the magnitude of penalties and should sum to 1``\n",
    "        gamma (Float): focal parameter\n",
    "                    ``control the balance between easy background and hard ROI training examples``\n",
    "    output:\n",
    "        tversky focal loss value with `mean` reduction\n",
    "    \"\"\"\n",
    "    return torch.mean(torch.pow(1 - tversky_index(yhat, ytrue, alpha, beta), gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "547eb8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(yhat, ytrue, alpha=0.75, gamma=2):\n",
    "    \"\"\"\n",
    "    Computes α-balanced focal loss from FAIR\n",
    "    https://arxiv.org/pdf/1708.02002v2.pdf\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks\n",
    "        ytrue (Tensor): targets masks\n",
    "        alpha (Float): weight to balance Cross entropy value\n",
    "        gamma (Float): focal parameter\n",
    "    output:\n",
    "        loss value with `mean` reduction\n",
    "    \"\"\"\n",
    "\n",
    "    # compute the actual focal loss\n",
    "    focal = -alpha * torch.pow(1. - yhat, gamma) * torch.log(yhat)\n",
    "    f_loss = torch.sum(ytrue * focal, dim=1)\n",
    "\n",
    "    return torch.mean(f_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c886b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Intersection_over_Union(yhat, ytrue, threshold=0.5, epsilon=1e-6, nan_process = 'remove'):\n",
    "    \"\"\"\n",
    "    Computes Intersection over Union metric\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks (batch_size, 1, height, width)\n",
    "        ytrue (Tensor): targets masks (batch_size, 1, height, width)\n",
    "        threshold (Float): threshold for pixel classification\n",
    "        epsilon (Float): smoothing parameter for numerical stability\n",
    "    output:\n",
    "        iou value with `mean` reduction\n",
    "    \"\"\"\n",
    "    intersection = ((yhat>threshold).long() & ytrue.long()).float().sum((1,2,3))\n",
    "    union = ((yhat>threshold).long() | ytrue.long()).float().sum((1,2,3))\n",
    "    if nan_process == 'remove': # if sum of true == 0, remove\n",
    "        sum_bool = torch.sum(torch.flatten(ytrue,1),1).bool()\n",
    "        iou =(intersection/(union))#.reshape(intersection.shape[0],-1)\n",
    "        iou = torch.nanmean(iou,dim=0)\n",
    "        if torch.isnan(iou):\n",
    "            return 0\n",
    "        return (torch.mean(iou)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0792005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dice_Coefficient(yhat, ytrue, epsilon=1e-6, nan_process = 'remove'):\n",
    "    \"\"\"\n",
    "    Computes a soft Dice Loss\n",
    "    Args:\n",
    "        yhat (Tensor): predicted masks (batch_size, 1, height, width)\n",
    "        ytrue (Tensor): targets masks (batch_size, 1, height, width)\n",
    "        epsilon (Float): smoothing value to avoid division by 0\n",
    "    output:\n",
    "        DL value with `mean` reduction\n",
    "    \"\"\"\n",
    "    # compute Dice components\n",
    "    intersection = torch.sum(yhat * ytrue, (1,2,3))\n",
    "    cardinal = torch.sum(yhat + ytrue, (1,2,3))\n",
    "    if nan_process == 'remove': # if sum of true == 0, remove\n",
    "        sum_bool = torch.sum(torch.flatten(ytrue,1),1).bool()\n",
    "        dice = (2 * intersection / (cardinal))#.reshape(intersection.shape[0],-1)\n",
    "        dice = torch.nanmean(dice,dim=0)\n",
    "        if torch.isnan(dice):\n",
    "            return 0\n",
    "        return (torch.mean(dice)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4711941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, epoch, \\\n",
    "          model, criterion, optimizer, device\n",
    "          ):\n",
    "    model.train()\n",
    "    limit=0\n",
    "    train_losses=AverageMeter()\n",
    "    for i, (input, target, _) in enumerate(train_loader):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        ss = model(input)\n",
    "        output = nn.ReLU()(model(input))\n",
    "#         print('input: ', input.size())\n",
    "#         print('target: ', target.size())\n",
    "#         print('ss: ', ss.size())\n",
    "#         print('output: ', output.size())\n",
    "        loss = criterion(output,target).float()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        train_losses.update(loss.detach().cpu().numpy(),input.shape[0])\n",
    "    Train_Loss=np.round(train_losses.avg,6)\n",
    "    return Train_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01ff039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, epoch, \\\n",
    "          model, criterion, optimizer, device\n",
    "          ):\n",
    "    model.train()\n",
    "    limit=0\n",
    "    train_losses=AverageMeter()\n",
    "    for i, (input, target, _) in enumerate(train_loader):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        ss = model(input)\n",
    "        output = nn.Sigmoid()(model(input))\n",
    "#         print('input: ', input.size())\n",
    "#         print('target: ', target.size())\n",
    "#         print('ss: ', ss.size())\n",
    "#         print('output: ', output.size())\n",
    "        loss = criterion(output,target).float()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        train_losses.update(loss.detach().cpu().numpy(),input.shape[0])\n",
    "    Train_Loss=np.round(train_losses.avg,6)\n",
    "    return Train_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4165d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(validation_loader, \n",
    "          model, criterion, device,\n",
    "        model_path=False,\n",
    "             return_image_paths=False,\n",
    "          ):\n",
    "    if model_path!=False:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    for i, (input, target, image_path) in enumerate(validation_loader):\n",
    "        input =input.to(device)\n",
    "        target = target.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = nn.Sigmoid()(model(input))\n",
    "#         print('output : ', output)\n",
    "        if i==0:\n",
    "#             print('i = 0')\n",
    "            targets=target\n",
    "            outputs=output\n",
    "            if return_image_paths==True:\n",
    "                image_paths = image_path\n",
    "        else:\n",
    "#             print('i  != 0')\n",
    "            targets=torch.cat((targets,target))\n",
    "            outputs=torch.cat((outputs,output),axis=0)\n",
    "            if return_image_paths==True:\n",
    "                image_paths += image_path\n",
    "    if return_image_paths==True:\n",
    "        return outputs, targets, image_paths\n",
    "    return outputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d77d5d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97471786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6221947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Do_Experiment(iteration, model_name, model, Optimizer, lr,  number_of_classes, epochs, Metrics,df,device):\n",
    "    train_dataset = ImagesDataset(train_image_path_list, train_target_path_list, transform)\n",
    "    validation_dataset = ImagesDataset(validation_image_path_list, validation_target_path_list, transform)\n",
    "    test_dataset = ImagesDataset(test_image_path_list, test_target_path_list, transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size,\n",
    "    num_workers=num_workers, pin_memory=pin_memory\n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, batch_size=batch_size, \n",
    "        num_workers=num_workers, pin_memory=pin_memory\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, \n",
    "        num_workers=num_workers, pin_memory=pin_memory\n",
    "    )\n",
    "    start = timeit.default_timer()\n",
    "    train_bool=True\n",
    "    test_bool=True\n",
    "    if loss_function == 'Tversky Focal Loss':\n",
    "        criterion=tversky_focal_loss\n",
    "    if Optimizer=='Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif Optimizer == 'SGD':\n",
    "        momentum = 0.9\n",
    "        weight_decay = 1e-4\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum ,weight_decay=weight_decay)\n",
    "    elif Optimizer =='AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    save_dir='saved_model'\n",
    "    try:\n",
    "        os.mkdir(save_dir)\n",
    "    except:\n",
    "        pass\n",
    "    if train_bool:\n",
    "        now = datetime.now()\n",
    "        Train_date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Training Start Time:',Train_date)\n",
    "        best=9999\n",
    "        best_epoch=1\n",
    "        Early_Stop=0\n",
    "        Early_Stop_Start=30\n",
    "        train_start_time = timeit.default_timer()\n",
    "        Train_Losses=[]\n",
    "        Validation_Losses=[]\n",
    "        for epoch in range(1, epochs+1):\n",
    "            adjust_learning_rate(optimizer, epoch, lr)\n",
    "            Train_Loss = train(train_loader, epoch, \n",
    "              model, criterion, optimizer, device\n",
    "              )\n",
    "            outputs, targets = validate(validation_loader, \n",
    "              model, criterion, device\n",
    "              )\n",
    "            Loss = np.round(criterion(outputs,targets).cpu().numpy(),6)            \n",
    "            iou = np.round(Intersection_over_Union(outputs, targets),3)\n",
    "            dice = np.round(Dice_Coefficient(outputs, targets),3)\n",
    "            now = datetime.now()\n",
    "            date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "            print(str(epoch)+'EP('+date+'):',end=' ')\n",
    "            print('T_Loss: ' + str(Train_Loss), end=' ')\n",
    "            print('V_Loss: ' + str(Loss), end=' ')\n",
    "            print('IoU: ' + str(iou), end=' ')\n",
    "            print('Dice: ' + str(dice), end='\\n')\n",
    "                        \n",
    "            if Loss<best:\n",
    "                torch.save(model.state_dict(), save_dir+'/'+model_name+'_'+Dataset_name+'.pt')\n",
    "                best_epoch = epoch\n",
    "                best = Loss\n",
    "                print('Best Epoch:',best_epoch,'Loss:',Loss)\n",
    "        train_stop_time = timeit.default_timer()\n",
    "    if test_bool:\n",
    "        now = datetime.now()\n",
    "        date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Test Start Time:',date)\n",
    "        outputs, targets, image_paths \\\n",
    "            = validate(test_loader, \n",
    "              model, criterion, device,\n",
    "            model_path=save_dir+'/'+model_name+'_'+Dataset_name+'.pt',\n",
    "                       return_image_paths=True\n",
    "              )        \n",
    "        Loss = np.round(criterion(outputs,targets).cpu().numpy(),6)\n",
    "        iou = np.round(Intersection_over_Union(outputs, targets),3)\n",
    "        dice = np.round(Dice_Coefficient(outputs, targets),3)\n",
    "\n",
    "        now = datetime.now()\n",
    "        date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "        print('Best Epoch:',best_epoch)\n",
    "        print('Test('+date+'): '+'Loss: ' + str(Loss),end=' ')\n",
    "        print('IoU: ' + str(iou), end=' ')\n",
    "        print('Dice: ' + str(dice), end='\\n')                    \n",
    "                            \n",
    "        stop = timeit.default_timer()\n",
    "        m, s = divmod((train_stop_time - train_start_time)/epoch, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        Time_per_Epoch = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "        m, s = divmod(stop - start, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        Time = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "        print(Time)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        total_params = format(total_params , ',')\n",
    "        Performances = [iteration, Dataset_name, model_name, loss_function, lr, batch_size, epochs,   Loss, iou, dice, total_params,Time, best_epoch, Time_per_Epoch]\n",
    "        Performances = pd.Series(Performances, index=df.columns)\n",
    "        df = pd.concat([df, Performances.to_frame().T], ignore_index=True)\n",
    "    now = datetime.now()\n",
    "    date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "    print('End',date)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48c16ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Start Time: 230717_202006\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "Experiment_date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "print('Experiment Start Time:',Experiment_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f5e653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 8\n",
      "Train Size 0.7\n"
     ]
    }
   ],
   "source": [
    "Metrics=['Iteration','Dataset','Model Name', 'Loss Function', 'LR', 'Batch size', '#Epochs',  'Loss', 'mIoU', 'mDice','Total Params','Train-Predction Time','Best Epoch','Time per Epoch']\n",
    "image_paths_dirs=[]\n",
    "target_dirs = [path  for path in natsort.natsorted(glob.glob(Dataset_dir+'/Masks/*'))]\n",
    "for target_dir in target_dirs:\n",
    "    image_paths_dirs.append(Dataset_dir+'/Originals/'+os.path.basename(target_dir).split('_')[0]+'_ori')\n",
    "df = pd.DataFrame(index=None, columns=Metrics)\n",
    "csv_file = False \n",
    "try:\n",
    "    if csv_file != False:\n",
    "        csv_file_for_modifying = 'Segmentation_Model_Comparison_Performance_220511_045617.csv'\n",
    "        df= pd.read_csv(csv_file_for_modifying, encoding='cp949')\n",
    "        df.to_csv('Segmentation_Model_Comparison_Performance_'+Experiment_date+'.csv', index=False, header=True, encoding=\"cp949\")\n",
    "except:\n",
    "    df = pd.DataFrame(index=None, columns=Metrics)\n",
    "print('Batch Size:',batch_size)\n",
    "print('Train Size',train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf5e49-3f14-4237-857f-10ec502f2540",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 reflex_mask\n",
      "SwinUNet\n",
      "LR: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userHome/userhome2/taekyung/miniconda3/envs/swin/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343962757/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start Time: 230717_202008\n",
      "1EP(230717_202035): T_Loss: 0.883739 V_Loss: 0.621754 IoU: 0.377 Dice: 0.502\n",
      "Best Epoch: 1 Loss: 0.621754\n",
      "2EP(230717_202058): T_Loss: 0.515631 V_Loss: 0.530357 IoU: 0.449 Dice: 0.591\n",
      "Best Epoch: 2 Loss: 0.530357\n",
      "3EP(230717_202121): T_Loss: 0.515094 V_Loss: 0.408654 IoU: 0.515 Dice: 0.656\n",
      "Best Epoch: 3 Loss: 0.408654\n",
      "4EP(230717_202145): T_Loss: 0.47457 V_Loss: 0.404955 IoU: 0.52 Dice: 0.665\n",
      "Best Epoch: 4 Loss: 0.404955\n",
      "5EP(230717_202212): T_Loss: 0.434913 V_Loss: 0.413397 IoU: 0.533 Dice: 0.681\n"
     ]
    }
   ],
   "source": [
    "Dataset_Number=0\n",
    "for image_paths_dir, target_dir in zip(image_paths_dirs, target_dirs):\n",
    "    Dataset_name = os.path.basename(target_dir)\n",
    "    if Dataset_name not in Target_Datasets:\n",
    "        continue\n",
    "    Dataset_Number+=1\n",
    "    print(str(Dataset_Number)+'/'+str(len(Target_Datasets)), Dataset_name)\n",
    "    for iteration in range(iterations[0], iterations[1]+1):\n",
    "        seed=iteration    \n",
    "        for model_name in model_names:\n",
    "            if len(df[(df['Dataset'] ==Dataset_name) & (df['Model Name']== model_name) & (df['Iteration']== iteration)])>0:\n",
    "                continue\n",
    "            image_path_list=natsort.natsorted(glob.glob(image_paths_dir+'/*'))\n",
    "            target_path_list=[]\n",
    "            for image_path in image_path_list:\n",
    "                #target_path_list.append(target_dir+'/'+os.path.basename(image_path).replace('npy','tif'))\n",
    "                target_path_list.append(target_dir+'/'+os.path.basename(image_path).replace('npy','png'))\n",
    "            transform = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                ])\n",
    "            num_workers=4\n",
    "            shuffle=True\n",
    "            pin_memory=True\n",
    "            num_dataset = len(target_path_list)\n",
    "            indices = list(range(num_dataset))\n",
    "            split1=int(train_size*num_dataset)\n",
    "            split2=int((train_size+(1-train_size)/2)*num_dataset)\n",
    "            control_random_seed(seed)\n",
    "            if shuffle:\n",
    "                np.random.shuffle(indices)\n",
    "            train_idx, validation_idx, test_idx = indices[:split1], indices[split1:split2], indices[split2:]\n",
    "            train_image_path_list=[]\n",
    "            train_target_path_list=[]\n",
    "            validation_image_path_list=[]\n",
    "            validation_target_path_list=[]\n",
    "            test_image_path_list=[]\n",
    "            test_target_path_list=[]\n",
    "            for i, index in enumerate(indices):\n",
    "                if i<split1:\n",
    "                    train_image_path_list.append(image_path_list[index])\n",
    "                    train_target_path_list.append(target_path_list[index])\n",
    "                elif split1<=i and i<split2:\n",
    "                    validation_image_path_list.append(image_path_list[index])\n",
    "                    validation_target_path_list.append(target_path_list[index])\n",
    "                else:\n",
    "                    test_image_path_list.append(image_path_list[index])\n",
    "                    test_target_path_list.append(target_path_list[index])\n",
    "\n",
    "            print(model_name)\n",
    "            for Optimizer in Optimizers :\n",
    "                for lr in LRs:\n",
    "                    print('LR:',lr)\n",
    "                    control_random_seed(seed)\n",
    "                    in_channels=3\n",
    "                    model=str_to_class(model_name)(in_channels, number_of_classes)\n",
    "#                     print('* model load success')\n",
    "#                     print(model)\n",
    "                    device = torch.device(\"cuda:\"+str(devices[0]))\n",
    "                    if len(devices)>1:\n",
    "                        model = torch.nn.DataParallel(model, device_ids = devices ).to(device)\n",
    "                    else:\n",
    "                        model = model.to(device)\n",
    "                    df = Do_Experiment(iteration, model_name, model, Optimizer, lr,  number_of_classes, epochs, Metrics,df,device)\n",
    "                    try:\n",
    "                        df.to_csv('Segmentation_Model_Comparison_Performance_'+Experiment_date+'.csv', index=False, header=True, encoding=\"cp949\")\n",
    "                    except:\n",
    "                        now = datetime.now()\n",
    "                        tmp_date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "                        df.to_csv('Segmentation_Model_Comparison_Performance_'+Experiment_date+'_'+tmp_date+'_tmp'+'.csv', index=False, header=True, encoding=\"cp949\")\n",
    "print('End')\n",
    "# os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4c1d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15946ec5",
   "metadata": {},
   "source": [
    "ws: 98"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7c6e80660a23bcb1844628affa9d9c7691ee9e0d7ba69225d29cb7614cfd8b12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
